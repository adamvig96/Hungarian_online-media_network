{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gspread\n",
    "\n",
    "import bs4\n",
    "\n",
    "import selenium\n",
    "\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = os.environ[\"VL_CONSTRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysqlclient\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/b3/a753b836eab49c865651eb2bc7203d070c58e5f22b33015b48fa6112bd7a/mysqlclient-1.4.6-cp37-cp37m-win_amd64.whl (262kB)\n",
      "Collecting psycopg2\n",
      "  Downloading https://files.pythonhosted.org/packages/83/8d/bbb2ca983f3939066e8d104fe7a7b0fce1fd3d0f706ddb6d8a86bb33f5da/psycopg2-2.8.5-cp37-cp37m-win_amd64.whl (1.1MB)\n",
      "Installing collected packages: mysqlclient, psycopg2\n",
      "Successfully installed mysqlclient-1.4.6 psycopg2-2.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mysqlclient psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### oldal amit használok 444.hu\n",
    "    page = requests.get('https://444.hu/')\n",
    "    ### leves, parsolás, html szövegként\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    ## a főoldalon nincsenek különböző hirblokkok, elég csak a központi blokkra  létrehozni egy listát amiben be vannak ágyazva a linkek a cikkekhez\n",
    "    weblinks_all = soup.find_all('div', class_=re.compile('row'))\n",
    "    ###Kiszedi az összes linket és berakja egy listába\n",
    "    pagelinks=[]\n",
    "    for weblinks in weblinks_all:\n",
    "        for item in weblinks.find_all('a'):\n",
    "            pagelinks.append(item.get('href'))\n",
    "    pagelinks=list(set(pagelinks))\n",
    "    ###létrehozok egy listát az elmúlt 4 npról megfelelő formátumban, hogy ki tudjam szűrni a releváns linkekeket\n",
    "    delay=[]\n",
    "    for i in range(4):\n",
    "        delay.append(arrow.now().shift(days=-i).format('YYYY/MM/DD'))\n",
    "    #létrehozom a végleges link listát, amiben csak azok szerepelnek, amik a 444-re vezetnek,  amikben dátum van, ezek a rendes cikkek és nem a comment szekcióra vezetnek át\n",
    "    pagelinks_final=[]\n",
    "    for n in range(len(delay)):\n",
    "        for i in range(len(pagelinks)):\n",
    "            if (delay[n] in pagelinks[i])==True and pagelinks[i][-9:]!='#comments' and pagelinks[i][:14]=='https://444.hu':\n",
    "                pagelinks_final.append(pagelinks[i])\n",
    "\n",
    "    # létrehozok egy lista az összes cikkről ami van a  főoldalonvan és leszedem a tartalmukat\n",
    "    soups=[]\n",
    "    for pagelink in pagelinks_final:\n",
    "        page=requests.get(pagelink)\n",
    "        soup=BeautifulSoup(page.content, 'html.parser')\n",
    "        soups.append(soup)\n",
    "    ## Csinálok egy listákból álló listát, aminek a hossza megegyezik a cikkek számával, minden cikkhez tartozik egy lista,\n",
    "    #ami tördelt részletekben tartalmmazza a cikkek szövegét, innentől kezdve megvan minden cikk szövege egy helyen\n",
    "    contents=[]\n",
    "    for i in range(len(soups)):\n",
    "        linkcontents=[]\n",
    "        for n in range(len(soups[i].find_all('article')[0].find_all('p'))):\n",
    "            if (soups[i].find_all('article')[0].find_all('p')[n].text)!='':\n",
    "                linkcontents.append(soups[i].find_all('article')[0].find_all('p')[n].text)\n",
    "        contents.append(linkcontents)\n",
    "\n",
    "\n",
    "    out= pd.DataFrame(list(zip(pagelinks_final,contents)), \n",
    "                   columns =['Link', 'Content'])\n",
    "    out['Page']=\"444.hu\"\n",
    "    out.to_sql(name=\"proba3\", con=constring, if_exists=\"replace\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
