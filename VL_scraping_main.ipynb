{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "import string\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "import arrow\n",
    "import sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = os.environ[\"VL_CONSTRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alap függvények"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(home_string, day_string):\n",
    "    page = requests.get(\"https://\" + home + \"/\", allow_redirects=False)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    l = []\n",
    "    for item in soup.find_all(\"a\"):\n",
    "        if type(item.get(\"href\")) == str:\n",
    "            if day_string in item.get(\"href\"):\n",
    "                if home_string in item.get(\"href\"):\n",
    "                    l.append(item.get(\"href\"))\n",
    "    links = list(set(l))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soups(page_links):\n",
    "    soups = []\n",
    "    for page_link in page_links:\n",
    "        page = requests.get(page_link, allow_redirects=False)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "        time.sleep(5)\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oldalak: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-86e6cc49677d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mlinkcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     for n in range(\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cikk-torzs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     ):\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msoups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cikk-torzs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#Index\n",
    "home = \"index.hu\"\n",
    "day = date.today().strftime(\"%%2F%Y%%2F%m%%2F%d\")\n",
    "links = get_links(home, day)\n",
    "\n",
    "index_links = []\n",
    "for i in range(len(links)):\n",
    "    if \"mindekozben\" not in links[i]:\n",
    "        index_links.append(links[i])\n",
    "\n",
    "soups = get_soups(index_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    for n in range(\n",
    "        len(soups[i].find(\"div\", class_=re.compile(\"cikk-torzs\")).find_all(\"p\"))\n",
    "    ):\n",
    "        if (soups[i].find(\"div\", class_=re.compile(\"cikk-torzs\")).find_all(\"p\")) != \"\":\n",
    "            linkcontents.append(\n",
    "                soups[i]\n",
    "                .find(\"div\", class_=re.compile(\"cikk-torzs\"))\n",
    "                .find_all(\"p\")[n]\n",
    "                .text\n",
    "            )\n",
    "    contents.append(\" \".join(linkcontents[1:]))\n",
    "\n",
    "index_out = pd.DataFrame(list(zip(index_links, contents)), columns=[\"Link\", \"Content\"])\n",
    "index_out['Page']=\"Index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#444\n",
    "home = \"444.hu\"\n",
    "day = date.today().strftime(\"%Y/%m/%d/\")\n",
    "negy_links = get_links(home, day)\n",
    "soups = get_soups(negy_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    for n in range(len(soups[i].find_all(\"article\")[0].find_all(\"p\"))):\n",
    "        if (soups[i].find_all(\"article\")[0].find_all(\"p\")[n].text) != \"\":\n",
    "            linkcontents.append(soups[i].find_all(\"article\")[0].find_all(\"p\")[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "negy_out = pd.DataFrame(list(zip(negy_links, contents)), columns=[\"Link\", \"Content\"])\n",
    "negy_out['Page']=\"444\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e18e96ccd70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlinkcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"article-menu_main\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#HVG\n",
    "home = \"hvg.hu\"\n",
    "day = date.today().strftime(\"%Y%m%d\")\n",
    "# day = '20200617'\n",
    "\n",
    "page = requests.get(\"https://\" + home + \"/\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "l = []\n",
    "for item in soup.find_all(\"a\"):\n",
    "    if type(item.get(\"href\")) == str:\n",
    "        if day in item.get(\"href\"):\n",
    "            l.append(item.get(\"href\"))\n",
    "links = list(set(l))\n",
    "hvg_links = []\n",
    "for link in links:\n",
    "    if \"360/\" not in link:\n",
    "        if \"https://\" not in link:\n",
    "            hvg_links.append(\"https://hvg.hu\" + link)\n",
    "\n",
    "soups = get_soups(hvg_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    soup = soups[i].find(\"div\", class_=re.compile(\"article-menu_main\")).find_all(\"p\")\n",
    "    for n in range(len(soup)):\n",
    "        if (soup) != \"\":\n",
    "            linkcontents.append(soup[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "hvg_out = pd.DataFrame(list(zip(hvg_links, contents)), columns=[\"Link\", \"Content\"])\n",
    "hvg_out['Page']=\"HVG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Origo\n",
    "home = \"origo.hu\"\n",
    "day = date.today().strftime(\"%Y%m%d\")\n",
    "# day = \"20200617\"\n",
    "origo_links = get_links(home, day)\n",
    "\n",
    "soups = get_soups(origo_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    soup = soups[i].find_all(\"p\")\n",
    "    for n in range(1, (len(soup))):\n",
    "        if (soup) != \"\":\n",
    "            linkcontents.append(soup[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "origo_out = pd.DataFrame(list(zip(origo_links, contents)), columns=[\"Link\", \"Content\"])\n",
    "origo_out['Page']=\"Origo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24.hu\n",
    "home = \"24.hu\"\n",
    "day = date.today().strftime(\"%Y/%m/%d/\")\n",
    "# day = \"2020/06/17\"\n",
    "huszon_links = get_links(home, day)\n",
    "\n",
    "soups = get_soups(huszon_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    soup = (\n",
    "        soups[i]\n",
    "        .find(\"div\", class_=re.compile(\"o-post__body o-postCnt post-body\"))\n",
    "        .find_all(\"p\")\n",
    "    )\n",
    "    for n in range(1, (len(soup))):\n",
    "        if (soup) != \"\":\n",
    "            linkcontents.append(soup[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "huszon_out = pd.DataFrame(\n",
    "    list(zip(huszon_links, contents)), columns=[\"Link\", \"Content\"]\n",
    ")\n",
    "huszon_out['Page']=\"24.hu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ripost\n",
    "page = requests.get(\"https://ripost.hu/\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "l = []\n",
    "for item in soup.find_all(\"a\"):\n",
    "    if type(item.get(\"href\")) == str:\n",
    "        if \"ripost.hu\" in item.get(\"href\"):\n",
    "            l.append(item.get(\"href\"))\n",
    "links = list(set(l))\n",
    "ripost_links = []\n",
    "for link in links:\n",
    "    try:\n",
    "        a = int(link[-8:-1])\n",
    "        ripost_links.append(link)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "soups = get_soups(ripost_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    soup = soups[i].find(\"div\", class_=re.compile(\"content-holder\")).find_all(\"p\")\n",
    "    for n in range(1, (len(soup))):\n",
    "        if (soup) != \"\":\n",
    "            linkcontents.append(soup[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "ripost_out = pd.DataFrame(\n",
    "    list(zip(ripost_links, contents)), columns=[\"Link\", \"Content\"]\n",
    ")\n",
    "ripost_out['Page']=\"Ripost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#888.hu\n",
    "page = requests.get(\"https://888.hu/\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "l = []\n",
    "for item in soup.find_all(\"a\"):\n",
    "    if type(item.get(\"href\")) == str:\n",
    "        if \"888.hu\" in item.get(\"href\"):\n",
    "            l.append(item.get(\"href\"))\n",
    "\n",
    "links = list(set(l))\n",
    "\n",
    "nyolc_links = []\n",
    "for link in links:\n",
    "    try:\n",
    "        a = int(link[-8:-1])\n",
    "        nyolc_links.append(link)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ezt csekkolni kell minden nap mi az új és csak azokat beletenni!\n",
    "\n",
    "soups = get_soups(nyolc_links)\n",
    "\n",
    "contents = []\n",
    "for i in range(len(soups)):\n",
    "    linkcontents = []\n",
    "    soup = soups[i].find(\"div\", class_=re.compile(\"maincontent8\")).find_all(\"p\")\n",
    "    for n in range(1, (len(soup))):\n",
    "        if (soup) != \"\":\n",
    "            linkcontents.append(soup[n].text)\n",
    "    contents.append(\" \".join(linkcontents))\n",
    "\n",
    "nyolc_out = pd.DataFrame(list(zip(nyolc_links, contents)), columns=[\"Link\", \"Content\"])\n",
    "nyolc_out['Page']=\"888\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts=pd.concat([index_out,negy_out,hvg_out,origo_out,huszon_out,ripost_out,nyolc_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_sql(name=\"VL_articles_main_v1\", con=constring, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table(\"VL_articles_main_v1\", con=constring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main=pd.read_sql_table(\"VL_articles_main_v1\", con=constring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main=main.drop_duplicates(subset='Link', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main..to_sql(name=\"VL_articles_main_v1\", con=constring, if_exists=\"replace\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
